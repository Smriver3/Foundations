---
title: "DA2_Homework_Classification_2"
output: pdf_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(rstan)
library(shinystan)
library(gridExtra)
library(caret)
library(cowplot)
library(DMwR)
set.seed(1234)

```

## Employee turnover 

We want to determine based whether or not an employee will leave *(i.e., turn)*. We'll build 2 models: a glm binomial regression, and we'll also build an logistic equation and compare results to glm. The dimensions we want to consider in the model are:  

* Satisfaction *(latest emp survey)*
* Last_Eval *(lastest evaluation)*
* Number_Projects *(average number of project per month)*
* Avg_Mo_Hrs *(average hours per month)*
* Tenure *(years with company)*
* Promotion *(promotion recieved in last year)*

First check the balance between 0 *(did not leave)* and 1 *(left)*

```{r, message=F, , echo=F, warning=F, fig.width=4, fig.height=3, fig.align="center"}

library(tidyverse)
library(rstan)
library(shinystan)
library(gridExtra)
library(caret)
library(cowplot)
library(DMwR)

set.seed(1231)

Emp_Turn <- read.csv("C:/Users/ellen/OneDrive/Documents/UH/Spring 2020/DA2/Section 1/Classification and SVM/Homework/EmpTurn2.csv")
Emp_Turn$Left <- factor(Emp_Turn$Left)
prop.table(table(Emp_Turn$Left))
```

It's impbalanced enough to consider runing SMOTE *(remember, the response variable needs to be a factor to run SMOTE, and needs to be numeric (0,1) to apply logistic regression analysis, so prepare to transform between operations)*

After applying SMOTE, the balance should be near the following *(play with the  perc.over and under to get what you want)*:

```{r, message=F, , echo=F, warning=F, fig.width=4, fig.height=3, fig.align="center"}

smoteData <- SMOTE(Left ~ ., Emp_Turn, perc.over = 350, perc.under=130) 
# SMOTE only works with facdtors
prop.table(table(smoteData$Left))
```

Now run glm to estimate your coefficients *(you want to use the smote data to train the model, but retain the orginal data for pulling testsets - keep the datatypes in sync!)*. 

```{r, message=F, , echo=F, warning=F, fig.width=4, fig.height=3, fig.align="center"}

smoteData$Left <- as.integer(smoteData$Left)-1
Emp_Turn$Left <- as.integer(Emp_Turn$Left)-1

# create training set (not going to use Test set yet)
xTrain <- dplyr::select(smoteData, Left, Satisfaction, Last_Eval, Number_Projects, Avg_Mo_Hrs, Tenure, Promotion)

glm.fit <- glm(Left ~ Satisfaction + Last_Eval + Number_Projects + Avg_Mo_Hrs + Tenure + Promotion, data = xTrain, family = binomial)
glm.fit$coefficients

```

Now, that you have coefficients, create a test file with 100 records *(just use sample_n, 100 on the original data)*. Using the glm coefficients, build a logistic regression equation, and calcuate probabilities *(write these to the test dataframe)*. Just for confidence, also run the test data through the glm fitted model and compare to your equation results to make sure all agree.

Now set all the records with a probabiliy over 50% to 1 *(Left)*, and use a confusion Matrix to score. 

```{r, message=F, , echo=F, warning=F, fig.width=4, fig.height=3, fig.align="center"}
alpha <- glm.fit$coefficients[1]
beta <- glm.fit$coefficients[2:7]


test <- sample_n(select(Emp_Turn, Left, Satisfaction, Last_Eval, Number_Projects, Avg_Mo_Hrs, Tenure, Promotion), 100)
tst1 <- data.matrix(select(test, -Left))
bet1 <- as.numeric(beta)
test$laProb <- exp(alpha[1] + t(bet1%*%t(tst1)))/(1+exp(alpha[1] + t(bet1%*%t(tst1))))

# score results
test$laLab <- ifelse(test$laProb < .5, 0, 1)
# check metrics

confusionMatrix(factor(test$laLab), factor(test$Left))

```

An accuracy score > 75% is fine *(there are ways we can impove this which we'll study later)*.

Finally, show the relationship between Employee Satisfaction and whether they left or not.
 
```{r, message=F, , echo=F, warning=F, fig.width=4, fig.height=3, fig.align="center"}

ggplot(test, aes(x = Satisfaction, y=laProb)) + geom_point() 

```

### Classification with Random Forest

Set up your 
